{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 8\n",
    "\n",
    "The first part of the assignment involves creating a Jupyter notebook that mimics a real-time streaming data feed. The basic loop for the notebook is simple. The notebook should load processed data and publish data at the appropriate time. You can use either the time given in the parquet partition or you can use the `offset` data found within the parquet data. For example, once your notebook has passed the 52.5-second mark it should load the data from the `t=052.5` directory and publish it to the appropriate Kafka topic. Similarly, you could example the `offset` column and publish the data at the appropriate time.  \n",
    "\n",
    "> **Hint**: You may want to use the Python [heapq](https://docs.python.org/3/library/heapq.html) library as an event queue.  \n",
    "\n",
    "The [DSC 650 Github contains example notebooks](https://github.com/bellevue-university/dsc650/tree/master/dsc650/assignments/assignment08) you can use to help you create topics, publish data to a Kafka broker, and consume the data.  \n",
    "\n",
    "Use the following parameters when publishing simulated data to the Bellevue University Data Science Cluster Kafka broker.  \n",
    "\n",
    "|                    |                                    |\n",
    "| :----------------- | :--------------------------------- |\n",
    "| Bootstrap Server   | kafka.kafka.svc.cluster.local:9092 |  \n",
    "| Location Topic     | LastnameFirstname-locations        |  \n",
    "| Acceleration Topic | LastnameFirstname-accelerations    |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is an example of code that uses the `kafka-python` library to publish a message to Kafka topic using a JSON serializer:  \n",
    "~~~python\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "bootstrap_server = 'kafka.kafka.svc.cluster.local:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "  bootstrap_servers=[bootstrap_server],\n",
    "  value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
    ")\n",
    "\n",
    "producer.send(\n",
    "  'DoeJohn-locations', \n",
    "  {\"dataObjectID\": \"test1\"}\n",
    ")\n",
    "~~~\n",
    "\n",
    "> **Hint**: When creating the notebook producer, you may want to automatically restart sending the data from the beginning when you reach the end of the dataset. This enables you to continue testing without having to manually restart the notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kafka.producer.future.FutureRecordMetadata at 0x7f3b1812e880>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap_server = 'kafka.kafka.svc.cluster.local:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "  bootstrap_servers=[bootstrap_server],\n",
    "  value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
    ")\n",
    "\n",
    "producer.send(\n",
    "  'DoeJohn-locations', \n",
    "  {\"dataObjectID\": \"test1\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "\n",
    "from kafka import KafkaProducer, KafkaAdminClient\n",
    "from kafka.admin.new_topic import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters \n",
    "\n",
    "> **TODO:** Change the configuration prameters to the appropriate values for your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    bootstrap_servers=['kafka.kafka.svc.cluster.local:9092'],\n",
    "    first_name='Scott',\n",
    "    last_name='Breitbach'\n",
    ")\n",
    "\n",
    "config['client_id'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "config['topic_prefix'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Topic Utility Function\n",
    "\n",
    "The `create_kafka_topic` helps create a Kafka topic based on your configuration settings.  For instance, if your first name is *John* and your last name is *Doe*, `create_kafka_topic('locations')` will create a topic with the name `DoeJohn-locations`.  The function will not create the topic if it already exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kafka_topic(topic_name, config=config, num_partitions=1, replication_factor=1):\n",
    "    bootstrap_servers = config['bootstrap_servers']\n",
    "    client_id = config['client_id']\n",
    "    topic_prefix = config['topic_prefix']\n",
    "    name = '{}-{}'.format(topic_prefix, topic_name)\n",
    "    \n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=bootstrap_servers, \n",
    "        client_id=client_id\n",
    "    )\n",
    "    \n",
    "    topic = NewTopic(\n",
    "        name=name,\n",
    "        num_partitions=num_partitions,\n",
    "        replication_factor=replication_factor\n",
    "    )\n",
    "\n",
    "    topic_list = [topic]\n",
    "    try:\n",
    "        admin_client.create_topics(new_topics=topic_list)\n",
    "        print('Created topic \"{}\"'.format(name))\n",
    "    except TopicAlreadyExistsError as e:\n",
    "        print('Topic \"{}\" already exists'.format(name))\n",
    "    \n",
    "create_kafka_topic('locations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set directories\n",
    "# current_dir = Path(os.getcwd()).absolute()\n",
    "# results_dir = current_dir.joinpath('results')\n",
    "# if results_dir.exists():\n",
    "#     shutil.rmtree(results_dir)\n",
    "# results_dir.mkdir(parents=True, exist_ok=True)\n",
    "src_data_path = '/home/jovyan/dsc650/data/processed/bdd/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is an example that uses the kafka-python library to consume messages from a Kafka topic. You should create another Jupyter notebook to consume messages from the Kafka producer to validate that you are properly publishing messages to the appropriate topic:  \n",
    "\n",
    "~~~python\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "bootstrap_server = 'kafka.kafka.svc.cluster.local:9092'\n",
    "\n",
    "# To consume latest messages and auto-commit offsets\n",
    "consumer = KafkaConsumer(\n",
    "    'DoeJohn-locations',\n",
    "    bootstrap_servers=[bootstrap_server]\n",
    ")\n",
    "~~~ \n",
    "\n",
    "> **Note**: While creating a separate notebook that acts as a Kafka consumer is not strictly necessary for the assignment, it is recommended that you create one to aid in debugging and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_server = 'kafka.kafka.svc.cluster.local:9092'\n",
    "\n",
    "# To consume latest messages and auto-commit offsets\n",
    "consumer = KafkaConsumer(\n",
    "    'DoeJohn-locations',\n",
    "    bootstrap_servers=[bootstrap_server]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you'd like to read the data from S3 instead, you can use this script:\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import heapq\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "s3 = s3fs.S3FileSystem(\n",
    "    anon=True,\n",
    "    client_kwargs={\n",
    "        'endpoint_url': endpoint_url\n",
    "    }\n",
    ")\n",
    "\n",
    "acceleration_columns = [\n",
    "    'offset',\n",
    "    'id',\n",
    "    'ride_id',\n",
    "    'uuid',\n",
    "    'x',\n",
    "    'y',\n",
    "    'z',\n",
    "#     't'\n",
    "]\n",
    "Acceleration = namedtuple('Acceleration', acceleration_columns)\n",
    "def read_accelerations():\n",
    "    df = pq.ParquetDataset(\n",
    "        's3://data/processed/bdd/accelerations',\n",
    "        filesystem=s3\n",
    "    ).read_pandas().to_pandas()\n",
    "    \n",
    "    df = df[acceleration_columns].sort_values(by=['offset'])\n",
    "    \n",
    "    records = [Acceleration(*record) for record in df.to_records(index=False)]\n",
    "    \n",
    "    return records\n",
    "accelerations = read_accelerations()\n",
    "\n",
    "location_columns = [\n",
    "    'offset',\n",
    "    'id',\n",
    "    'ride_id',\n",
    "    'uuid',\n",
    "    'course',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'geohash',\n",
    "    'speed',\n",
    "    'accuracy',\n",
    "#     't'\n",
    "]\n",
    "Location = namedtuple('Location', location_columns)\n",
    "def read_locations():\n",
    "    df = pq.ParquetDataset(\n",
    "        's3://data/processed/bdd/locations',\n",
    "        filesystem=s3\n",
    "    ).read_pandas().to_pandas()\n",
    "    \n",
    "    df = df[location_columns].sort_values(by=['offset'])\n",
    "    \n",
    "    records = [Location(*record) for record in df.to_records(index=False)]\n",
    "    \n",
    "    return records\n",
    "    \n",
    "locations = read_locations("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### organize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pyarrow.parquet as pq\n",
    "from collections import namedtuple\n",
    "# Set path to data\n",
    "src_data_path = '/home/jovyan/dsc650/data/processed/bdd/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load acceleration data into pandas dataframe\n",
    "accel_df = pq.ParquetDataset(\n",
    "    src_data_path + 'accelerations/').read_pandas().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'ride_id', 'uuid', 'timestamp', 'offset', 'x', 'y', 'z',\n",
       "       'timelapse', 'filename', 't'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accel_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "accel_cols = [\n",
    "    'offset',\n",
    "    'ride_id',\n",
    "    'uuid', \n",
    "    'timestamp', \n",
    "    'x', 'y', 'z',\n",
    "    'timelapse', \n",
    "    'filename', \n",
    "    't'\n",
    "]\n",
    "\n",
    "# Order df by specified columns & sort by offset value\n",
    "accel_df = accel_df[accel_cols].sort_values(by=['offset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>ride_id</th>\n",
       "      <th>uuid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>timelapse</th>\n",
       "      <th>filename</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.822061</td>\n",
       "      <td>c9a2b46c9aa515b632eddc45c4868482</td>\n",
       "      <td>19b9aa10588646b3bf22c9b4865a7995</td>\n",
       "      <td>1970-01-01 00:25:03.882586</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>False</td>\n",
       "      <td>e2f795a7-6a7d-4500-b5d7-4569de996811.mov</td>\n",
       "      <td>000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     offset                           ride_id  \\\n",
       "0  0.822061  c9a2b46c9aa515b632eddc45c4868482   \n",
       "\n",
       "                               uuid                  timestamp      x      y  \\\n",
       "0  19b9aa10588646b3bf22c9b4865a7995 1970-01-01 00:25:03.882586 -0.994  0.045   \n",
       "\n",
       "       z  timelapse                                  filename      t  \n",
       "0 -0.036      False  e2f795a7-6a7d-4500-b5d7-4569de996811.mov  000.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accel_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>ride_id</th>\n",
       "      <th>uuid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>timelapse</th>\n",
       "      <th>filename</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23351</th>\n",
       "      <td>122.469896</td>\n",
       "      <td>7758b047316155c7991ceddfcb964f96</td>\n",
       "      <td>6cfaa4a137fc419199221719e2b34aae</td>\n",
       "      <td>1970-01-01 00:25:03.500103</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>False</td>\n",
       "      <td>ef07c198-5de5-430f-b648-632c762c7b3a.mov</td>\n",
       "      <td>121.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           offset                           ride_id  \\\n",
       "23351  122.469896  7758b047316155c7991ceddfcb964f96   \n",
       "\n",
       "                                   uuid                  timestamp      x  \\\n",
       "23351  6cfaa4a137fc419199221719e2b34aae 1970-01-01 00:25:03.500103 -0.983   \n",
       "\n",
       "          y      z  timelapse                                  filename      t  \n",
       "23351 -0.02 -0.091      False  ef07c198-5de5-430f-b648-632c762c7b3a.mov  121.4  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accel_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # accel_df.columns\n",
    "# # Select columns\n",
    "# accel_cols = [\n",
    "#     'offset',\n",
    "#     'id',\n",
    "#     'ride_id',\n",
    "#     'uuid',\n",
    "#     'x',\n",
    "#     'y',\n",
    "#     'z',\n",
    "# #     't'\n",
    "# ]\n",
    "\n",
    "# # Filter df for selected columns & sort by offset value\n",
    "# accel_df = accel_df[accel_cols].sort_values(by=['offset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define named tuple\n",
    "# Accelerations = namedtuple('Accelerations', accel_cols)\n",
    "\n",
    "# # Assign records to named tuple\n",
    "# records = [Accelerations(*record) for record in accel_df.to_records(index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0779125295566454"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[0].offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load location data into pandas dataframe\n",
    "locat_df = pq.ParquetDataset(\n",
    "    src_data_path + 'locations/').read_pandas().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'ride_id', 'uuid', 'timestamp', 'offset', 'course', 'latitude',\n",
       "       'longitude', 'geohash', 'speed', 'accuracy', 'timelapse', 'filename',\n",
       "       't'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locat_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "locat_cols = [\n",
    "    'offset',\n",
    "    'id', \n",
    "    'ride_id', \n",
    "    'uuid', \n",
    "    'timestamp', \n",
    "    'course', \n",
    "    'latitude',\n",
    "    'longitude', \n",
    "    'geohash', \n",
    "    'speed', \n",
    "    'accuracy', \n",
    "    'timelapse', \n",
    "    'filename',\n",
    "    't'\n",
    "]\n",
    "\n",
    "# Order df by specified columns & sort by offset value\n",
    "locat_df = locat_df[locat_cols].sort_values(by=['offset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # locat_df.columns\n",
    "# # Select columns\n",
    "# locat_cols = [\n",
    "#     'offset',\n",
    "#     'id',\n",
    "#     'ride_id',\n",
    "#     'uuid',\n",
    "#     'course',\n",
    "#     'latitude',\n",
    "#     'longitude',\n",
    "#     'geohash',\n",
    "#     'speed',\n",
    "#     'accuracy',\n",
    "# #     't'\n",
    "# ]\n",
    "\n",
    "# # Filter df for selected columns & sort by offset value\n",
    "# locat_df = locat_df[locat_cols].sort_values(by=['offset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define named tuple\n",
    "# Locations = namedtuple('Locations', locat_cols)\n",
    "\n",
    "# # Assign records to named tuple\n",
    "# records = [Locations(*record) for record in locat_df.to_records(index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23512"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accel_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(locat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>id</th>\n",
       "      <th>ride_id</th>\n",
       "      <th>uuid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>course</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>geohash</th>\n",
       "      <th>speed</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>timelapse</th>\n",
       "      <th>filename</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.077913</td>\n",
       "      <td>85c61911b7fe2ced1000c33c9e932706</td>\n",
       "      <td>6760ffa3f41908695d1405b776c3e8d5</td>\n",
       "      <td>dad7eae44e784b549c8c5a3aa051a8c7</td>\n",
       "      <td>1970-01-01 00:25:07.320453</td>\n",
       "      <td>158.203125</td>\n",
       "      <td>40.677641</td>\n",
       "      <td>-73.817930</td>\n",
       "      <td>dr5x2jpkmtcy</td>\n",
       "      <td>2.120000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>d745b92f-aefd-467d-9121-7a71308e8d6d.mov</td>\n",
       "      <td>000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.525061</td>\n",
       "      <td>58682c5d48cad9d9e103431d773615bf</td>\n",
       "      <td>c9a2b46c9aa515b632eddc45c4868482</td>\n",
       "      <td>19b9aa10588646b3bf22c9b4865a7995</td>\n",
       "      <td>1970-01-01 00:25:03.882586</td>\n",
       "      <td>299.619141</td>\n",
       "      <td>40.762870</td>\n",
       "      <td>-73.961949</td>\n",
       "      <td>dr5ruuwscttz</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>e2f795a7-6a7d-4500-b5d7-4569de996811.mov</td>\n",
       "      <td>000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.525061</td>\n",
       "      <td>58682c5d48cad9d9e103431d773615bf</td>\n",
       "      <td>c9a2b46c9aa515b632eddc45c4868482</td>\n",
       "      <td>19b9aa10588646b3bf22c9b4865a7995</td>\n",
       "      <td>1970-01-01 00:25:03.882583</td>\n",
       "      <td>299.619141</td>\n",
       "      <td>40.762870</td>\n",
       "      <td>-73.961949</td>\n",
       "      <td>dr5ruuwsctv3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>e2f795a7-6a7d-4500-b5d7-4569de996811.mov</td>\n",
       "      <td>004.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.077913</td>\n",
       "      <td>85c61911b7fe2ced1000c33c9e932706</td>\n",
       "      <td>6760ffa3f41908695d1405b776c3e8d5</td>\n",
       "      <td>dad7eae44e784b549c8c5a3aa051a8c7</td>\n",
       "      <td>1970-01-01 00:25:07.320449</td>\n",
       "      <td>159.960938</td>\n",
       "      <td>40.677883</td>\n",
       "      <td>-73.818047</td>\n",
       "      <td>dr5x2jpmfffw</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>d745b92f-aefd-467d-9121-7a71308e8d6d.mov</td>\n",
       "      <td>004.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.077913</td>\n",
       "      <td>85c61911b7fe2ced1000c33c9e932706</td>\n",
       "      <td>6760ffa3f41908695d1405b776c3e8d5</td>\n",
       "      <td>dad7eae44e784b549c8c5a3aa051a8c7</td>\n",
       "      <td>1970-01-01 00:25:07.320446</td>\n",
       "      <td>159.609375</td>\n",
       "      <td>40.678191</td>\n",
       "      <td>-73.818193</td>\n",
       "      <td>dr5x2jppxkqj</td>\n",
       "      <td>13.150000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>d745b92f-aefd-467d-9121-7a71308e8d6d.mov</td>\n",
       "      <td>007.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>121.440927</td>\n",
       "      <td>064184d44429da1967d8b2873d6c5053</td>\n",
       "      <td>6974a887428b102c9564dbcc41b52df0</td>\n",
       "      <td>f57cdc42893d4cbc8b139c1e1650482f</td>\n",
       "      <td>1970-01-01 00:25:04.256850</td>\n",
       "      <td>173.629547</td>\n",
       "      <td>32.172733</td>\n",
       "      <td>34.893325</td>\n",
       "      <td>sv8z40bec2ht</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>e85b702f-9912-4899-9e10-ccd89606f089.mov</td>\n",
       "      <td>121.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>121.927121</td>\n",
       "      <td>None</td>\n",
       "      <td>b710d8312757efeb9d4d1dbf0e545653</td>\n",
       "      <td>95ffd0af905a4549bfe975526605e41f</td>\n",
       "      <td>1970-01-01 00:24:50.489901</td>\n",
       "      <td>296.872864</td>\n",
       "      <td>37.804754</td>\n",
       "      <td>-122.272297</td>\n",
       "      <td>9q9p1dhh8uzj</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>faa2aa67-b813-4d82-a1e1-4e308be8086d.mov</td>\n",
       "      <td>121.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>122.166896</td>\n",
       "      <td>193ec33b0d4330452d8e6214388efbe6</td>\n",
       "      <td>7758b047316155c7991ceddfcb964f96</td>\n",
       "      <td>6cfaa4a137fc419199221719e2b34aae</td>\n",
       "      <td>1970-01-01 00:25:03.500104</td>\n",
       "      <td>200.039062</td>\n",
       "      <td>40.683265</td>\n",
       "      <td>-74.001895</td>\n",
       "      <td>dr5rkn1mny7g</td>\n",
       "      <td>19.049999</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>ef07c198-5de5-430f-b648-632c762c7b3a.mov</td>\n",
       "      <td>121.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>122.420328</td>\n",
       "      <td>4a7bf56df03b1db624c2813b6b94dfda</td>\n",
       "      <td>3b210670ccfce98e494c10d1c5a70ca7</td>\n",
       "      <td>d1f37820e0e842069d59649cae7cd0ef</td>\n",
       "      <td>1970-01-01 00:25:07.065369</td>\n",
       "      <td>159.372467</td>\n",
       "      <td>40.923765</td>\n",
       "      <td>-73.857733</td>\n",
       "      <td>dr72z3fqnndx</td>\n",
       "      <td>4.930000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>False</td>\n",
       "      <td>92d1b741-3093-4991-9b35-ba406aa55d78.mov</td>\n",
       "      <td>121.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>122.440927</td>\n",
       "      <td>064184d44429da1967d8b2873d6c5053</td>\n",
       "      <td>6974a887428b102c9564dbcc41b52df0</td>\n",
       "      <td>f57cdc42893d4cbc8b139c1e1650482f</td>\n",
       "      <td>1970-01-01 00:25:04.256849</td>\n",
       "      <td>170.899216</td>\n",
       "      <td>32.172800</td>\n",
       "      <td>34.893312</td>\n",
       "      <td>sv8z40bs1he4</td>\n",
       "      <td>5.830000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>e85b702f-9912-4899-9e10-ccd89606f089.mov</td>\n",
       "      <td>121.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>478 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         offset                                id  \\\n",
       "1      1.077913  85c61911b7fe2ced1000c33c9e932706   \n",
       "0      1.525061  58682c5d48cad9d9e103431d773615bf   \n",
       "2      4.525061  58682c5d48cad9d9e103431d773615bf   \n",
       "3      5.077913  85c61911b7fe2ced1000c33c9e932706   \n",
       "5      8.077913  85c61911b7fe2ced1000c33c9e932706   \n",
       "..          ...                               ...   \n",
       "476  121.440927  064184d44429da1967d8b2873d6c5053   \n",
       "473  121.927121                              None   \n",
       "472  122.166896  193ec33b0d4330452d8e6214388efbe6   \n",
       "475  122.420328  4a7bf56df03b1db624c2813b6b94dfda   \n",
       "477  122.440927  064184d44429da1967d8b2873d6c5053   \n",
       "\n",
       "                              ride_id                              uuid  \\\n",
       "1    6760ffa3f41908695d1405b776c3e8d5  dad7eae44e784b549c8c5a3aa051a8c7   \n",
       "0    c9a2b46c9aa515b632eddc45c4868482  19b9aa10588646b3bf22c9b4865a7995   \n",
       "2    c9a2b46c9aa515b632eddc45c4868482  19b9aa10588646b3bf22c9b4865a7995   \n",
       "3    6760ffa3f41908695d1405b776c3e8d5  dad7eae44e784b549c8c5a3aa051a8c7   \n",
       "5    6760ffa3f41908695d1405b776c3e8d5  dad7eae44e784b549c8c5a3aa051a8c7   \n",
       "..                                ...                               ...   \n",
       "476  6974a887428b102c9564dbcc41b52df0  f57cdc42893d4cbc8b139c1e1650482f   \n",
       "473  b710d8312757efeb9d4d1dbf0e545653  95ffd0af905a4549bfe975526605e41f   \n",
       "472  7758b047316155c7991ceddfcb964f96  6cfaa4a137fc419199221719e2b34aae   \n",
       "475  3b210670ccfce98e494c10d1c5a70ca7  d1f37820e0e842069d59649cae7cd0ef   \n",
       "477  6974a887428b102c9564dbcc41b52df0  f57cdc42893d4cbc8b139c1e1650482f   \n",
       "\n",
       "                     timestamp      course   latitude   longitude  \\\n",
       "1   1970-01-01 00:25:07.320453  158.203125  40.677641  -73.817930   \n",
       "0   1970-01-01 00:25:03.882586  299.619141  40.762870  -73.961949   \n",
       "2   1970-01-01 00:25:03.882583  299.619141  40.762870  -73.961949   \n",
       "3   1970-01-01 00:25:07.320449  159.960938  40.677883  -73.818047   \n",
       "5   1970-01-01 00:25:07.320446  159.609375  40.678191  -73.818193   \n",
       "..                         ...         ...        ...         ...   \n",
       "476 1970-01-01 00:25:04.256850  173.629547  32.172733   34.893325   \n",
       "473 1970-01-01 00:24:50.489901  296.872864  37.804754 -122.272297   \n",
       "472 1970-01-01 00:25:03.500104  200.039062  40.683265  -74.001895   \n",
       "475 1970-01-01 00:25:07.065369  159.372467  40.923765  -73.857733   \n",
       "477 1970-01-01 00:25:04.256849  170.899216  32.172800   34.893312   \n",
       "\n",
       "          geohash      speed  accuracy  timelapse  \\\n",
       "1    dr5x2jpkmtcy   2.120000      10.0      False   \n",
       "0    dr5ruuwscttz   0.000000      10.0      False   \n",
       "2    dr5ruuwsctv3   0.000000      10.0      False   \n",
       "3    dr5x2jpmfffw  11.750000      10.0      False   \n",
       "5    dr5x2jppxkqj  13.150000      10.0      False   \n",
       "..            ...        ...       ...        ...   \n",
       "476  sv8z40bec2ht   5.900000       5.0      False   \n",
       "473  9q9p1dhh8uzj   0.000000      10.0      False   \n",
       "472  dr5rkn1mny7g  19.049999       5.0      False   \n",
       "475  dr72z3fqnndx   4.930000      30.0      False   \n",
       "477  sv8z40bs1he4   5.830000       5.0      False   \n",
       "\n",
       "                                     filename      t  \n",
       "1    d745b92f-aefd-467d-9121-7a71308e8d6d.mov  000.0  \n",
       "0    e2f795a7-6a7d-4500-b5d7-4569de996811.mov  000.0  \n",
       "2    e2f795a7-6a7d-4500-b5d7-4569de996811.mov  004.5  \n",
       "3    d745b92f-aefd-467d-9121-7a71308e8d6d.mov  004.5  \n",
       "5    d745b92f-aefd-467d-9121-7a71308e8d6d.mov  007.8  \n",
       "..                                        ...    ...  \n",
       "476  e85b702f-9912-4899-9e10-ccd89606f089.mov  121.4  \n",
       "473  faa2aa67-b813-4d82-a1e1-4e308be8086d.mov  121.4  \n",
       "472  ef07c198-5de5-430f-b648-632c762c7b3a.mov  121.4  \n",
       "475  92d1b741-3093-4991-9b35-ba406aa55d78.mov  121.4  \n",
       "477  e85b702f-9912-4899-9e10-ccd89606f089.mov  121.4  \n",
       "\n",
       "[478 rows x 14 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### okay, so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pyarrow.parquet as pq\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import time\n",
    "from kafka import KafkaProducer, KafkaAdminClient\n",
    "from kafka.admin.new_topic import NewTopic\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "# Set path to data\n",
    "src_data_path = '/home/jovyan/dsc650/data/processed/bdd/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load acceleration data into pandas dataframe\n",
    "accel_df = pq.ParquetDataset(\n",
    "    src_data_path + 'accelerations/').read_pandas().to_pandas()\n",
    "\n",
    "# Reorder columns\n",
    "accel_cols = [\n",
    "    'offset',\n",
    "    'ride_id',\n",
    "    'uuid', \n",
    "    'timestamp', \n",
    "    'x', 'y', 'z',\n",
    "    'timelapse', \n",
    "    'filename', \n",
    "    't'\n",
    "]\n",
    "\n",
    "# Order df by specified columns & sort by offset value\n",
    "accel_df = accel_df[accel_cols].sort_values(by=['offset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load location data into pandas dataframe\n",
    "locat_df = pq.ParquetDataset(\n",
    "    src_data_path + 'locations/').read_pandas().to_pandas()\n",
    "\n",
    "# Reorder columns\n",
    "locat_cols = [\n",
    "    'offset',\n",
    "    'id', \n",
    "    'ride_id', \n",
    "    'uuid', \n",
    "    'timestamp', \n",
    "    'course', \n",
    "    'latitude',\n",
    "    'longitude', \n",
    "    'geohash', \n",
    "    'speed', \n",
    "    'accuracy', \n",
    "    'timelapse', \n",
    "    'filename',\n",
    "    't'\n",
    "]\n",
    "\n",
    "# Order df by specified columns & sort by offset value\n",
    "locat_df = locat_df[locat_cols].sort_values(by=['offset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infos = []\n",
    "# offsets = []\n",
    "# topic = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ~convert timestamp to string~\n",
    "nvm, using JSON instead of dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accel_df['timestamp'] = accel_df['timestamp'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"offset\":0.8220608865,\"ride_id\":\"c9a2b46c9aa515b632eddc45c4868482\",\"uuid\":\"19b9aa10588646b3bf22c9b4865a7995\",\"timestamp\":1503882,\"x\":-0.994,\"y\":0.045,\"z\":-0.036,\"timelapse\":false,\"filename\":\"e2f795a7-6a7d-4500-b5d7-4569de996811.mov\",\"t\":\"000.0\"}'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accel_df.iloc[0].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loc = locat_df.head()\n",
    "test_acc = accel_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['offset', 'topic', 'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "offset                                        1.07791\n",
       "id                   85c61911b7fe2ced1000c33c9e932706\n",
       "ride_id              6760ffa3f41908695d1405b776c3e8d5\n",
       "uuid                 dad7eae44e784b549c8c5a3aa051a8c7\n",
       "timestamp                  1970-01-01 00:25:07.320453\n",
       "course                                        158.203\n",
       "latitude                                      40.6776\n",
       "longitude                                    -73.8179\n",
       "geohash                                  dr5x2jpkmtcy\n",
       "speed                                            2.12\n",
       "accuracy                                           10\n",
       "timelapse                                       False\n",
       "filename     d745b92f-aefd-467d-9121-7a71308e8d6d.mov\n",
       "t                                               000.0\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_loc.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['offset', 'id', 'ride_id', 'uuid', 'timestamp', 'course', 'latitude',\n",
       "       'longitude', 'geohash', 'speed', 'accuracy', 'timelapse', 'filename',\n",
       "       't'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_loc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>topic</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.077913</td>\n",
       "      <td>locations</td>\n",
       "      <td>{'offset': 1.0779125295566454, 'id': '85c61911...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.525061</td>\n",
       "      <td>locations</td>\n",
       "      <td>{'offset': 1.525060886522843, 'id': '58682c5d4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.525061</td>\n",
       "      <td>locations</td>\n",
       "      <td>{'offset': 4.5250608865228426, 'id': '58682c5d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.077913</td>\n",
       "      <td>locations</td>\n",
       "      <td>{'offset': 5.077912529556645, 'id': '85c61911b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.077913</td>\n",
       "      <td>locations</td>\n",
       "      <td>{'offset': 8.077912529556645, 'id': '85c61911b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     offset      topic                                               data\n",
       "0  1.077913  locations  {'offset': 1.0779125295566454, 'id': '85c61911...\n",
       "0  1.525061  locations  {'offset': 1.525060886522843, 'id': '58682c5d4...\n",
       "0  4.525061  locations  {'offset': 4.5250608865228426, 'id': '58682c5d...\n",
       "0  5.077913  locations  {'offset': 5.077912529556645, 'id': '85c61911b...\n",
       "0  8.077913  locations  {'offset': 8.077912529556645, 'id': '85c61911b..."
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = 'locations'\n",
    "for i in range(len(test_loc)):\n",
    "    ofst = test_loc.offset.iloc[i]\n",
    "    data = test_loc.iloc[i].to_dict()\n",
    "    df2 = pd.DataFrame([[ofst, topic, data]], columns=cols)\n",
    "#     print(df2)\n",
    "#     df.append(df2)\n",
    "    df = pd.concat([df, df2])\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### and then..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_data(df, topic):\n",
    "    cols = ['offset', 'topic', 'data']\n",
    "    temp_df = pd.DataFrame(columns=cols)\n",
    "    for row in range(len(df)):\n",
    "        record_offset = df.offset.iloc[row]\n",
    "        record_data = df.iloc[row].to_json() # changed from .to_dict()\n",
    "        record = pd.DataFrame([[record_offset, topic, record_data]], \n",
    "                              columns=cols)\n",
    "        temp_df = pd.concat([temp_df, record])\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_ref = reformat_data(test_acc, 'accelerations')\n",
    "# # acc_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc_ref = reformat_data(test_loc, 'locations')\n",
    "# # loc_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([acc_ref, loc_ref]).sort_values('offset', ignore_index=True)\n",
    "# # df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ref = reformat_data(accel_df, 'accelerations')\n",
    "loc_ref = reformat_data(locat_df, 'locations')\n",
    "df = pd.concat([acc_ref, loc_ref]).sort_values('offset', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>topic</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.822061</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{\"offset\":0.8220608865,\"ride_id\":\"c9a2b46c9aa5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.842061</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{\"offset\":0.8420608865,\"ride_id\":\"c9a2b46c9aa5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.862061</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{\"offset\":0.8620608865,\"ride_id\":\"c9a2b46c9aa5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.882061</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{\"offset\":0.8820608865,\"ride_id\":\"c9a2b46c9aa5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.902061</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{\"offset\":0.9020608865,\"ride_id\":\"c9a2b46c9aa5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23985</th>\n",
       "      <td>122.449896</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{\"offset\":122.4498959608,\"ride_id\":\"7758b04731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23986</th>\n",
       "      <td>122.453927</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{\"offset\":122.4539268052,\"ride_id\":\"6974a88742...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23987</th>\n",
       "      <td>122.455121</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{\"offset\":122.4551214097,\"ride_id\":\"b710d83127...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23988</th>\n",
       "      <td>122.465328</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{\"offset\":122.4653281476,\"ride_id\":\"3b210670cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23989</th>\n",
       "      <td>122.469896</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{\"offset\":122.4698959608,\"ride_id\":\"7758b04731...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23990 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           offset          topic  \\\n",
       "0        0.822061  accelerations   \n",
       "1        0.842061  accelerations   \n",
       "2        0.862061  accelerations   \n",
       "3        0.882061  accelerations   \n",
       "4        0.902061  accelerations   \n",
       "...           ...            ...   \n",
       "23985  122.449896  accelerations   \n",
       "23986  122.453927  accelerations   \n",
       "23987  122.455121  accelerations   \n",
       "23988  122.465328  accelerations   \n",
       "23989  122.469896  accelerations   \n",
       "\n",
       "                                                    data  \n",
       "0      {\"offset\":0.8220608865,\"ride_id\":\"c9a2b46c9aa5...  \n",
       "1      {\"offset\":0.8420608865,\"ride_id\":\"c9a2b46c9aa5...  \n",
       "2      {\"offset\":0.8620608865,\"ride_id\":\"c9a2b46c9aa5...  \n",
       "3      {\"offset\":0.8820608865,\"ride_id\":\"c9a2b46c9aa5...  \n",
       "4      {\"offset\":0.9020608865,\"ride_id\":\"c9a2b46c9aa5...  \n",
       "...                                                  ...  \n",
       "23985  {\"offset\":122.4498959608,\"ride_id\":\"7758b04731...  \n",
       "23986  {\"offset\":122.4539268052,\"ride_id\":\"6974a88742...  \n",
       "23987  {\"offset\":122.4551214097,\"ride_id\":\"b710d83127...  \n",
       "23988  {\"offset\":122.4653281476,\"ride_id\":\"3b210670cc...  \n",
       "23989  {\"offset\":122.4698959608,\"ride_id\":\"7758b04731...  \n",
       "\n",
       "[23990 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# test = [1, 1, 4, 5, 8]\n",
    "# time_start = time.time()\n",
    "# for i in test:\n",
    "#     while (time.time() - time_start < i):\n",
    "#         pass\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### producer time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap_servers': ['kafka.kafka.svc.cluster.local:9092'],\n",
       " 'first_name': 'Scott',\n",
       " 'last_name': 'Breitbach',\n",
       " 'client_id': 'BreitbachScott',\n",
       " 'topic_prefix': 'BreitbachScott'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Set configuration parameters\n",
    "config = dict(\n",
    "    bootstrap_servers=['kafka.kafka.svc.cluster.local:9092'],\n",
    "    first_name='Scott',\n",
    "    last_name='Breitbach'\n",
    ")\n",
    "\n",
    "config['client_id'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "config['topic_prefix'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a kafka topic based on config settings\n",
    "def create_kafka_topic(topic_name, config=config, num_partitions=1, replication_factor=1):\n",
    "    bootstrap_servers = config['bootstrap_servers']\n",
    "    client_id = config['client_id']\n",
    "    topic_prefix = config['topic_prefix']\n",
    "    name = '{}-{}'.format(topic_prefix, topic_name)\n",
    "    \n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=bootstrap_servers, \n",
    "        client_id=client_id\n",
    "    )\n",
    "    \n",
    "    topic = NewTopic(\n",
    "        name=name,\n",
    "        num_partitions=num_partitions,\n",
    "        replication_factor=replication_factor\n",
    "    )\n",
    "\n",
    "    topic_list = [topic]\n",
    "    try:\n",
    "        admin_client.create_topics(new_topics=topic_list)\n",
    "        print('Created topic \"{}\"'.format(name))\n",
    "    except TopicAlreadyExistsError as e:\n",
    "        print('Topic \"{}\" already exists'.format(name))\n",
    "    \n",
    "# create_kafka_topic('locations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin.new_topic import NewTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic \"BreitbachScott-accelerations\"\n",
      "Created topic \"BreitbachScott-locations\"\n"
     ]
    }
   ],
   "source": [
    "## Create Kafka topics\n",
    "create_kafka_topic('accelerations')\n",
    "create_kafka_topic('locations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaAdminClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Serialize Python objects as JSON\n",
    "producer = KafkaProducer(\n",
    "  bootstrap_servers=config['bootstrap_servers'],\n",
    "  value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Send Python objects to a Kafka topic\n",
    "def on_send_success(record_metadata):\n",
    "    print('Message sent:\\n    Topic: \"{}\"\\n    Partition: {}\\n    Offset: {}'.format(\n",
    "        record_metadata.topic,\n",
    "        record_metadata.partition,\n",
    "        record_metadata.offset\n",
    "    ))\n",
    "    \n",
    "def on_send_error(excp):\n",
    "    print('I am an errback', exc_info=excp)\n",
    "    # handle exception\n",
    "\n",
    "def send_data(topic, data, config=config, producer=producer, msg_key=None):\n",
    "    topic_prefix = config['topic_prefix']\n",
    "    topic_name = '{}-{}'.format(topic_prefix, topic)\n",
    "    \n",
    "    if msg_key is not None:\n",
    "        key = msg_key\n",
    "    else:\n",
    "        key = uuid.uuid4().hex\n",
    "    \n",
    "    producer.send(\n",
    "        topic_name, \n",
    "        value=data,\n",
    "        key=key.encode('utf-8')\n",
    "    ).add_callback(on_send_success).add_errback(on_send_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>topic</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.822061</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{\"offset\":0.8220608865,\"ride_id\":\"c9a2b46c9aa5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     offset          topic                                               data\n",
       "0  0.822061  accelerations  {\"offset\":0.8220608865,\"ride_id\":\"c9a2b46c9aa5..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8420608865228429\n",
      "accelerations\n",
      "{\"offset\":0.8420608865,\"ride_id\":\"c9a2b46c9aa515b632eddc45c4868482\",\"uuid\":\"19b9aa10588646b3bf22c9b4865a7995\",\"timestamp\":1503882,\"x\":-0.998,\"y\":0.046,\"z\":-0.04,\"timelapse\":false,\"filename\":\"e2f795a7-6a7d-4500-b5d7-4569de996811.mov\",\"t\":\"000.0\"}\n"
     ]
    }
   ],
   "source": [
    "test_offset = df.offset.iloc[loc]\n",
    "print(test_offset)\n",
    "test_topic = df.topic.iloc[loc]\n",
    "print(test_topic)\n",
    "test_data = df.data.iloc[loc]\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message sent:\n",
      "    Topic: \"BreitbachScott-accelerations\"\n",
      "    Partition: 0\n",
      "    Offset: 1\n"
     ]
    }
   ],
   "source": [
    "send_data(test_topic, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    record_offset = df.offset.iloc[i]\n",
    "    while (time.time() - time_start) < record_offset:\n",
    "        pass\n",
    "    send_data(df.topic.iloc[i], df.data.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "for i in range(len(test)):\n",
    "    j = test['offset'].iloc[i]\n",
    "    while (time.time() - time_start) < j:\n",
    "        pass\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
